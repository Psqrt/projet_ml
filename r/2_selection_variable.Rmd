---
title: "Sélection des variables"
date: "Étape 3"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo    = TRUE,
               prompt  = FALSE,
               tidy    = FALSE,
               comment = NA)
opts_knit$set(width = 75)
```

# Importation des Packages et fonctions externes

```{r, message=F, warning=F}
source("./0_packages.R")
```

# Importation des données

```{r}
type_col = readRDS("./../data/data_imp_type_col.rds")
data_imp = fread(file = "./../data/data_imp.csv", colClasses = type_col) %>% as.data.frame()
```

# LGBM pour sélectionner les variables

L'utilisation d'un LGBM pour la sélection de variable est incitée pour deux raisons :

* La rapidité d'exécution pour un modèle complexe
* La possibilité de se servir du modèle afin de comparer le coefficient normalisé de Gini avant et après la première phase de traitement.

Les hyperparamètres restent les mêmes qu'avant, de même pour la stratégie de validation du modèle (5-folds CV).

```{r}
# Préparation des données pour le LGBM
train_X_lgb  = data_imp %>% 
  filter(dataset == "train") %>% 
  select(-id, -target, -dataset) %>% 
  lgb.prepare() %>% 
  as.matrix()

train_Y_lgb = data_imp %>% 
  filter(dataset == "train") %>% 
  select(target) %>% 
  lgb.prepare() %>% 
  as.matrix() - 1

train_Y_num = as.numeric(data_imp$target[data_imp$dataset == "train"]) - 1

test_X_lgb   = data_imp %>% 
  filter(dataset == "test") %>% 
  select(-id, -target, -dataset) %>% 
  lgb.prepare() %>% 
  as.matrix() 

test_Y_lgb = data_imp %>% 
  filter(dataset == "test") %>% 
  select(target) %>% 
  lgb.prepare() %>% 
  as.matrix()

test_Y_num = as.numeric(data_imp$target[data_imp$dataset == "test"]) - 1

train_XY_lgb = lgb.Dataset(train_X_lgb, label = train_Y_lgb)

# Préparation des données pour le RF
train_XY = data_imp %>% 
  filter(dataset == "train") %>% 
  select(-dataset, -id)

test_X = data_imp %>% 
  filter(dataset == "test") %>% 
  select(-dataset, -id, -target)
```

## Cross Validation

```{r}
params_lgb = list(
  objective        = "binary", # type of exercise
  metric           = "auc",    # metric to be evaluated
  learning_rate    = 0.01,     # shrinkage rate
  max_depth        = 10,       # max depth for tree model (used to deal with over-fitting when data is small)
  num_leaves       = 20,       # max number of leaves (nodes) in one tree
  is_unbalance     = T,        # is data unbalanced
  min_data_in_leaf = 1,        # min number of data in one leaf (used to deal with over-fitting)
  feature_fraction = 0.8,      # randomly select part of the features on each iteration
  bagging_fraction = 0.8,      # randomly select part of the data without resampling
  bagging_freq     = 5,        # if != 0, enables bagging, performs bagging at every k iteration
  num_threads      = 6         # number of cpu cores (not threads) to use
)
```

```{r, eval = F}
set.seed(1234)
selection_lgb_cv = lgb.cv(
  params                = params_lgb,         # hyperparameters
  data                  = train_XY_lgb,       # lgb.Dataset object for training
  eval                  = lgb.normalizedgini, # custom metric, additionnal to first metric
  nrounds               = 1000,               # maximum iterations
  early_stopping_rounds = 50,                 # if metric evaluation doesn't increase
  verbose               = 1,                  # enable verbose
  eval_freq             = 50,                 # verbose every n iterations
  nfold                 = 5                   # k-folds CV
)
```

## Modèle LGBM optimisé

```{r, eval = F}
selection_lgb_model <- lgb.train(
  params    = params_lgb,                 # hyperparameters
  data      = train_XY_lgb,               # lgb.Dataset object for training
  valids    = list(train = train_XY_lgb), # lgb.Dataset object for validation
  eval      = lgb.normalizedgini,         # custom metric, additionnal to first metric
  nrounds   = selection_lgb_cv$best_iter,           # nrounds from CV
  verbose   = 1,                          # enable verbose
  eval_freq = 50                          # verbose every n iterations
)
```

```{r}
# save(selection_lgb_cv, file = "./../data/selection_lgb_cv.RData")
# lgb.save(selection_lgb_model, filename = "./../data/selection_lgb_model.txt")
load("./../data/selection_lgb_cv.RData")
selection_lgb_model = lgb.load("./../data/selection_lgb_model.txt")
```

## Variable importance

```{r}
fun_imp_ggplot_split_boosting(selection_lgb_model, "Gain")
```

## Performance - LGBM

```{r}
selection_lgb_train_preds  = predict(selection_lgb_model, train_X_lgb)
selection_lgb_test_preds   = predict(selection_lgb_model, test_X_lgb)
selection_lgb_train_ngini  = normalizedGini(train_Y_num, selection_lgb_train_preds)
selection_lgb_test_ngini   = normalizedGini(test_Y_num, selection_lgb_test_preds)
selection_lgb_valid_ngini  = max(unlist(selection_lgb_cv[["record_evals"]][["valid"]][["Norm-gini"]][["eval"]]))
c("Normalized Gini Coeff. (Train)"        = selection_lgb_train_ngini,
  "Normalized Gini Coeff. (Valid - 5fCV)" = selection_lgb_valid_ngini,
  "Normalized Gini Coeff. (Test)"         = selection_lgb_test_ngini)
```

On note que le coefficient normalisé de Gini a été amélioré par rapport au LGBM de benchmark, le modèle actuel a même dépassé le benchmark par XGBM.

# Forêt aléatoire pour sélection de variables

La forêt aléatoire est construite sans paramétrisation. On prend des valeurs fixes.

```{r, eval = F}
set.seed(1234)
selection_rf_model = ranger(target ~ .,
                            data = train_XY,
                            num.trees = 500,            # nombre d'arbres
                            sample.fraction = 0.7,      # échantillonnage
                            importance = "permutation", # type d'importance
                            probability = T)            # conserver les probabilités
```

```{r}
# save(selection_rf_model, file = "./../data/selection_rf_model.RData")
load("./../data/selection_rf_model.RData")
```

## Variable importance

```{r}
fun_imp_ggplot_split(selection_rf_model)
```

## Performance - RF

```{r}
selection_rf_train_preds          = predict(selection_rf_model, train_XY)$predictions[, 2]
selection_rf_test_preds           = predict(selection_rf_model, test_X)$predictions[, 2]
selection_rf_train_ngini          = normalizedGini(train_Y_num, selection_rf_train_preds)
selection_rf_test_ngini           = normalizedGini(test_Y_num, selection_rf_test_preds)
selection_rf_valid_ngini          = normalizedGini(train_Y_num, selection_rf_model$predictions[,2])
c("Normalized Gini Coeff. (Train)"        = selection_rf_train_ngini,
  "Normalized Gini Coeff. (Valid - 5kCV)" = selection_rf_valid_ngini,
  "Normalized Gini Coeff. (Test)"         = selection_rf_test_ngini)
```

Le but de la forêt aléatoire est de sélectionner des variables et non d'en faire un benchmark. Les résultats ont été affichés par simple curiosité.

# Sélection des variables

On ne garde que les variables importantes (la première moitié pour chaque algorithme). On prendra alors l'union des deux listes proposées.

```{r}
# récupération des variables importantes du LGBM
var_imp_lgb = selection_lgb_model %>% 
  lgb.importance() %>% 
  as.data.frame() %>% 
  arrange(-Gain) %>% 
  slice(1:floor(nrow(.)/2)) %>% 
  select(Feature) %>% 
  .$Feature

# récupération des variables importantes de la RF
var_imp_rf = selection_rf_model$variable.importance %>% 
  sort(decreasing = T) %>% 
  .[1:floor(length(.)/2)] %>% 
  names()

# On prend l'union des deux listes
var_imp_both = c(var_imp_lgb, var_imp_rf) %>% unique()

# Puis on supprime les variables non retenues
data_sel = data_imp %>% 
  select(dataset, id, target, var_imp_both)
```

On retient 35 features.

```{r}
# On nettoie tout pour poursuivre le traitement ...
rm(list = ls()[grep("_rf", ls())])
gc()
```

## Visualisation des distributions

On représente la distribution de chaque variable.

```{r}
liste_gg = list()
for (i in c(4:ncol(data_sel))) {
  
  class_var = class(data_sel[[i]])    
  
  if (class_var == "factor"){
    gg = data_sel %>% 
      filter(dataset == "train") %>% 
      ggplot() +
      aes_string(x = colnames(data_sel)[i]) + 
      geom_bar() +
      ylab(NULL) +
      theme(axis.text = element_blank())
  } else {
    gg = data_sel %>% 
      filter(dataset == "train") %>% 
      ggplot() +
      aes_string(x = colnames(data_sel)[i]) + 
      geom_density() +
      ylab(NULL) +
      theme(axis.text = element_blank())
  }
  liste_gg[[i]] = gg
}

liste_gg = liste_gg[4:length(liste_gg)]

ggarrange(plotlist = liste_gg, ncol = 6, nrow = 6)
```

# Benchmark - LGBM

Après avoir supprimé les variables, on peut refaire un benchmark pour voir l'évolution après cette étape de sélection.

```{r}
# Prépration des données pour le LGBM
train_X_lgb  = data_sel %>% 
  filter(dataset == "train") %>% 
  select(-id, -target, -dataset) %>% 
  lgb.prepare() %>% 
  as.matrix()

train_Y_lgb = data_sel %>% 
  filter(dataset == "train") %>% 
  select(target) %>% 
  lgb.prepare() %>% 
  as.matrix() - 1

train_Y_num = as.numeric(data_sel$target[data_sel$dataset == "train"]) - 1

test_X_lgb   = data_sel %>% 
  filter(dataset == "test") %>% 
  select(-id, -target, -dataset) %>% 
  lgb.prepare() %>% 
  as.matrix() 

test_Y_lgb = data_sel %>% 
  filter(dataset == "test") %>% 
  select(target) %>% 
  lgb.prepare() %>% 
  as.matrix()

test_Y_num = as.numeric(data_sel$target[data_sel$dataset == "test"]) - 1

train_XY_lgb = lgb.Dataset(train_X_lgb, label = train_Y_lgb)
```

```{r, eval = F}
set.seed(1234)
post_selection_lgb_cv = lgb.cv(
  params                = params_lgb,         # hyperparameters
  data                  = train_XY_lgb,       # lgb.Dataset object for training
  eval                  = lgb.normalizedgini, # custom metric, additionnal to first metric
  nrounds               = 1000,               # maximum iterations
  early_stopping_rounds = 50,                 # if metric evaluation doesn't increase
  verbose               = 1,                  # enable verbose
  eval_freq             = 50,                 # verbose every n iterations
  nfold                 = 5                   # k-folds CV
)

post_selection_lgb_model <- lgb.train(
  params    = params_lgb,                      # hyperparameters
  data      = train_XY_lgb,                    # lgb.Dataset object for training
  valids    = list(train = train_XY_lgb),      # lgb.Dataset object for validation
  eval      = lgb.normalizedgini,              # custom metric, additionnal to first metric
  nrounds   = post_selection_lgb_cv$best_iter, # nrounds from CV
  verbose   = 1,                               # enable verbose
  eval_freq = 50                               # verbose every n iterations
)
```

```{r}
# save(post_selection_lgb_cv, file = "./../data/post_selection_lgb_cv.RData")
# lgb.save(post_selection_lgb_model, filename = "./../data/post_selection_lgb_model.txt")
load("./../data/post_selection_lgb_cv.RData")
post_selection_lgb_model = lgb.load("./../data/post_selection_lgb_model.txt")
```

```{r}
post_selection_lgb_train_preds          = predict(post_selection_lgb_model, train_X_lgb)
post_selection_lgb_test_preds           = predict(post_selection_lgb_model, test_X_lgb)
post_selection_lgb_train_ngini          = normalizedGini(train_Y_num, post_selection_lgb_train_preds)
post_selection_lgb_test_ngini           = normalizedGini(test_Y_num, post_selection_lgb_test_preds)
post_selection_leg_valid_ngini          = max(unlist(post_selection_lgb_cv[["record_evals"]][["valid"]][["Norm-gini"]][["eval"]]))
c("Normalized Gini Coeff. (Train)"        = post_selection_lgb_train_ngini,
  "Normalized Gini Coeff. (Valid - 5fCV)" = post_selection_leg_valid_ngini,
  "Normalized Gini Coeff. (Test)"         = post_selection_lgb_test_ngini)
```

Le score a encore été amélioré.

## Exportation

On exporte la base avec les variables retenues.

```{r, eval = F}
type_col = sapply(data_sel, class)

saveRDS(object = type_col,
        file = "./../data/data_sel_type_col.rds")
write.csv(x = data_sel,
          file = "./../data/data_sel.csv",
          quote = T,
          row.names = F)
```

