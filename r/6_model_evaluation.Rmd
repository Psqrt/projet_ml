---
title: "Évaluation des modèles"
date: "Étape 6"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print = "75")
opts_chunk$set(echo    = TRUE,
               prompt  = FALSE,
               tidy    = FALSE,
               comment = NA)
opts_knit$set(width = 75)
```

# Importation des Packages et fonctions externes

```{r, message=F, warning=F}
source("./0_packages.R")

# library(doMC)
# registerDoMC(cores = 1)
```

# Importation des données

```{r}
load("./../data/bases_35features.RData")
load("./../data/none_lgb_info.RData") # performance du LGB
load("./../data/up_lgb_info.RData") # performance du LGB avec scale_pos_weight

train_id = train$id
train_XY = train %>% select(-id)
train_X  = train %>% select(-id, -target)
train_Y  = train$target %>% as.numeric()
train_Y  = train_Y - 1

test_id = test$id
test_X  = test %>% select(-id, -target)
test_Y  = test$target %>% as.numeric()
test_Y  = test_Y - 1
```

# Zone de stockage

Les modèles sont stockés en dehors du répertoire github car trop lourds.

```{r}
path_exportation = file.path("/home/psqrt/Téléchargements/000")
```

# Importation / Calcul de la performance (gini) + ROC de tous les modèles

```{r, eval = F}
df_score_modeles = data.frame("Method"     = character(),
                              "Sampling"   = character(),
                              "Train"      = numeric(),
                              "Validation" = numeric(),
                              "Test"       = numeric(),
                              stringsAsFactors = F)

df_roc_modeles = data.frame("Obs" = train_Y,
                            stringsAsFactors = F)



donnees_modeles = list.files(path_exportation)
donnees_modeles = donnees_modeles[-grep("ada|kppv|txt", donnees_modeles)] # on ne prend pas ada et kppv (trop lents)


for (rdata_file in donnees_modeles){
  modele = gsub(".RData", "", rdata_file)
  print(modele)
  load(file.path(path_exportation, rdata_file))
  
  param_tune = unlist(get(modele)$bestTune)
  scores_cv = get(modele)$pred
  
  if (grepl("rf", modele)){
    param_tune[grep("splitrule", names(param_tune))] = "gini"
  }
  for (i in 1:length(param_tune)){
    print(names(param_tune)[i])
    scores_cv = scores_cv %>% 
      filter(get(names(param_tune)[i]) == param_tune[i])
  }
  scores_cv = scores_cv %>% 
    arrange(rowIndex) %>% 
    select(yes)
  colnames(scores_cv) = modele
  
  df_roc_modeles = df_roc_modeles %>% 
    bind_cols(scores_cv)
  rm(list = get("modele"))
}
```

Il manque les prédictions light gbm (none et up).

L'objet lgb.cv ne contenant pas les prédictions CV de la meilleur itération (959ème) mais uniquement la dernière (1000ème), on relance le LGB avec les paramètres tunés et la meilleure itération comme dernière itération à faire. La dernière itération sera donc la meilleure itération.

```{r, eval = F}
set.seed(1234)
params_lgb = list(
  objective        = "binary",                            # type of exercise
  metric           = "auc",                               # metric to be evaluated
  learning_rate    = 0.01,                                # shrinkage rate
  max_depth        = up_lgb_best_tune$max_depth,          # max depth for tree model (used to deal with over-fitting)
  num_leaves       = up_lgb_best_tune$num_leaves,         # max number of leaves (nodes) in one tree
  scale_pos_weight = 26,                                  # weight positive class
  min_data_in_leaf = 1,                                   # min number of data in one leaf (used to deal with over-fitting)
  feature_fraction = up_lgb_best_tune$feature_fraction,   # randomly select part of the features per iteration
  bagging_fraction = up_lgb_best_tune$bagging_fraction,   # randomly select part of the data without resampling
  bagging_freq     = 5,                                   # if != 0, enables bagging, performs bagging at every k iteration
  num_threads      = 6                                    # number of cpu cores (not threads) to use
)

up_lgb_cv_stacking = lgb.cv(
  params                = params_lgb,         # hyperparameters
  data                  = train_XY_lgb,       # lgb.Dataset object for training
  eval                  = lgb.normalizedgini, # custom metric, additionnal to first metric
  nrounds               = 536,                # maximum iterations (obtenir 536 en itérant jusqu'à 1000 puis placer la bonne valeur)
  early_stopping_rounds = 50,                 # if metric evaluation doesn't increase
  verbose               = 1,                  # enable verbose
  eval_freq             = 50,                 # verbose every n iterations
  nfold                 = 5                   # k-folds CV
)
```

```{r}
# save(up_lgb_cv_stacking, file = "./../data/up_lgb_cv_stacking.RData")
load("./../data/up_lgb_cv_stacking.RData")
```


On récupère les prédictions CV.

Source pour la fonction d'extraction des prédictions : https://github.com/Microsoft/LightGBM/issues/283

```{r}
up_lgb_cv_stacking_valid_preds = get_lgbm_cv_preds(up_lgb_cv_stacking)

# on enregistre les prédictions pour la courbe roc plus tard
# save(up_lgb_cv_stacking_valid_preds, file = "./../data/up_lgb_stacking_valid_preds.RData")
```

On rajoute les deux colonnes de prédictions (une pour up, une pour none) à la table commune.

```{r}
load("./../data/none_lgb_stacking_valid_preds.RData")
load("./../data/up_lgb_stacking_valid_preds.RData")
load("./../data/none_glm_stacking_valid_preds.RData")
load("./../data/none_combi_valid_preds.RData")
```

```{r, eval = F}
df_roc_modeles = df_roc_modeles %>% 
  bind_cols("none_lgb_model"      = none_lgb_cv_stacking_valid_preds) %>% 
  bind_cols("up_lgb_model"        = up_lgb_cv_stacking_valid_preds) %>% 
  bind_cols("none_glmstack_model" = none_glm_stacking_valid_preds) %>% 
  bind_cols("none_combi_model"    = none_combi_valid_preds)

# save(df_roc_modeles, file = "./../data/df_roc_modeles.RData")
```


```{r}
load("./../data/df_roc_modeles.RData") # info pour roc curve

df_roc = df_roc_modeles %>% 
  # select(contains("none"), Obs) %>% 
  gather(key = "Method", value = "score", -Obs) %>% 
  mutate(Sampling = sub("_.*", "", Method)) %>% 
  mutate(Algorithme = stringr::str_match(Method, ".*_(.*?)_model")[, 2]) 
  
#tout
df_roc %>% ggplot() +
  aes(d = Obs, m = score, color = Algorithme) +
  geom_roc() +
  theme(legend.position = "bottom") +
  facet_wrap(~ Sampling, nrow = 2)

#none
df_roc %>% 
  filter(Sampling == "none") %>% 
  ggplot() +
  aes(d = Obs, m = score, color = Algorithme) +
  geom_roc(size = 0.5, labels = F) +
  xlab("Taux de Faux Positifs") +
  ylab("Taux de Vrais Positifs") +
  ggtitle("Courbes ROC", subtitle = "Prédictions 5fCV des modèles entraînés sans resampling")
```




```{r}
# save(df_score_modeles, file = "./../data/df_score_modeles.RData")

load("./../data/df_score_modeles.RData")                         # info de tous les modèles sauf ...
load("./../data/none_lgb_info.RData")                          # info lgb none
load("./../data/up_lgb_info.RData")                            # info lgb up
load("./../data/glm_stacking_model_info.RData")                # info stacking
load("./../data/combinaison_model_info.RData")                 # info combi
load(file.path(path_exportation, "none_ada_model_trick.RData"))  # none_ada_best_gini
load(file.path(path_exportation, "smote_ada_model_trick.RData")) # smote_ada_best_gini
load(file.path(path_exportation, "down_ada_model_trick.RData"))  # down_ada_best_gini 

ada_lignes_df = data.frame("Method"     = c("ada", "ada", "ada"),
                           "Sampling"   = c("none", "smote", "down"),
                           "Train"      = rep(NA, 3),
                           "Validation" = c(none_ada_best_gini, smote_ada_best_gini, down_ada_best_gini),
                           "Test"       = rep(NA, 3),
                           stringsAsFactors = F)

df_score_modeles = df_score_modeles %>% 
  bind_rows(none_lgb_info) %>% 
  bind_rows(up_lgb_info) %>% 
  bind_rows(ada_lignes_df) %>% 
  bind_rows(glm_stacking_model_info) %>% 
  bind_rows(combinaison_model_info)
```




```{r}
df_score_modeles %>% 
  filter(Validation > 0.23) %>% # on retire les modèles trop mauvais
  ggplot() +
  aes(x = Sampling, y = Validation, group = Sampling, label = Method) +
  geom_hline(yintercept = 0.2721429, color = "red", linetype = 2) +
  stat_boxplot(geom = "errorbar", aes(color = Sampling)) +
  geom_boxplot(aes(color = Sampling)) +
  geom_point() +
  geom_text_repel(direction = "x", nudge_x = -0.25) +
  coord_flip() +
  ylim(c(0.24, 0.29)) +
  xlab("Sampling Strategy") +
  ylab("Normalized Gini Coefficient (5 folds CV)") +
  annotation_custom(grob = textGrob(label = "LGBM benchmark", 
                                    gp = gpar(fontsize = 9, col = "red")), 
                    xmin = -3.5, 
                    ymin = 0.262) +
  theme_bw() +
  theme(legend.position = "none") +
  ggtitle(label = "Comparaison des performances (validation croisée 5 blocs)", 
          subtitle = "6 Modèles ont été retirés avant le construction des boxplots car trop mauvais \nglm_stack = stacking de XGB et LGB avec un GLM /// combi = 0.1 * pred_LGB + 0.9 * pred_XGB")
```

## Modèle retenu

Le modèle retenu est le combiné. Le gini associé à l'échantillon test est :

```{r}
cat("Coefficient Normalisé de Gini sur l'échantillon test :\n", df_score_modeles$Test[which(df_score_modeles$Method == "combi")])
```

Les prédictions en CV sont récupérées. Maintenant il s'agit de voir quel threshold appliquer pour maximiser un objectif (F1 par exemple).

```{r}
df_threshold = data.frame("target" = train_Y,
                          "pred_combi" = none_combi_valid_preds)

df_f1 = data.frame("threshold" = seq(0, 0.25, 0.001),
                   "F1" = as.numeric(NA))
for (threshold in seq(0, 0.25, 0.001)){
  df_f1[which(df_f1$threshold == threshold), "F1"] = f1Score(df_threshold$target, df_threshold$pred_combi, cutoff = threshold)
}

df_f1 %>% 
  ggplot() +
  aes(x = threshold,
      y = F1) +
  geom_point()
```

```{r}
best_threshold_cv = df_f1 %>% 
  arrange(-F1) %>% 
  slice(1)

df_threshold = df_threshold %>% 
  mutate(label = if_else(pred_combi > best_threshold_cv$threshold, 1, 0))

caret::confusionMatrix(factor(df_threshold$label), factor(df_threshold$target), positive = "1", mode = "everything")
```

On atteint un score F1 de 0.118 sachant qu'on arrive à identifier correctement un label positif sur 4 en réalité. En appliquant ce même threshold pour l'échantillon test, on trouve :

```{r}
load("./../data/none_combi_test_preds.RData")
df_threshold_test = data.frame("target" = test_Y,
                               "pred_combi" = df_test_combi$combinaison)

df_threshold_test = df_threshold_test %>% 
  mutate(label = if_else(pred_combi > best_threshold_cv$threshold, 1, 0))

caret::confusionMatrix(factor(df_threshold_test$label), factor(df_threshold_test$target), positive = "1", mode = "everything")
```

On trouve un score F1 de 0.1158 pour le test, et un Recall de 0.2488. Les résultats sont stables entre la validation et le test.
